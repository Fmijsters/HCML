{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMSxpJAkqYzk"
   },
   "source": [
    "# Programming Assignment II: Explainability\n",
    "\n",
    "In this assignment you will train machine learning models and experiment with techniques discussed in the lectures.\n",
    "This assignment makes use of existing Python libraries. We have provided links to tutorials/examples if you're not familiar with them yet.\n",
    "\n",
    "\n",
    "All code that you write should be in this notebook. You should submit:\n",
    "* This notebook with your code added. Make sure to add enough documentation.\n",
    "* A short report, max 3 pages including any figures and/or tables. Use this [template](https://www.overleaf.com/read/mvskntycrckw).\n",
    "* Zip the notebook .ipynb and report .pdf files in a file with name format 'Prog_Explainability_Group_[X].zip', where X is your programming group ID (e.g. Prog_Explainability_Group_10.zip). The .ipynb and .pdf files should also have the same name as the zip file.\n",
    "\n",
    "\n",
    "Important notes:\n",
    "* Deadline for this assignment is **Monday June 7, 17:00**. \n",
    "* Send it to both Heysem Kaya (h.kaya@uu.nl) and Yupei Du (y.du@uu.nl), CCing your programming partner.\n",
    "* Title of the email: [INFOMHCML] Explainability programming assignment submission [X], with X the number of your group.\n",
    "* There will be a lab session to assist you with the assignment on **Tuesday, June 1, between 9:00-12:45 over Lab Channel in Teams**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moyaViIx8WzS"
   },
   "source": [
    "## Installation\n",
    "\n",
    "For this assignment, we are going to use the following Python packages:\n",
    "\n",
    "matplotlib, pandas, statsmodels, interpret, scikit-learn, openpyxl and graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6EaC6P7RqXOh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Installing packages\n",
    "!conda install python-graphviz\n",
    "!pip install matplotlib pandas statsmodels interpret sklearn openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeSC0_WEpY0k"
   },
   "source": [
    "## Downloading the data\n",
    "We are going to use the combined cycle power plant dataset. This dataset contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006-2011), when the power plant was set to work with full load. We have the following features: hourly average ambient variables Temperature (T), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V). We will train ML models to predict the net hourly electrical energy output (EP) of the plant.\n",
    "\n",
    "For a detailed description, see: [[Description](https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant)]\n",
    "\n",
    "We first need to download and prepare data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fleSmPrE7UMT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "unzip:  cannot find or open CCPP.zip, CCPP.zip.zip or CCPP.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# Download and unzip data\n",
    "!wget -c https://archive.ics.uci.edu/ml/machine-learning-databases/00294/CCPP.zip\n",
    "!unzip CCPP.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQpW5C3Sg9YA"
   },
   "source": [
    "## Loading and preprocessing the data\n",
    "We split the data into training (first 5000 instances) and validation (the subsequent 2000) and test (the last 2568) sets. We will use the training set to train a model, and validation set to optimize the model hyper-parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "JycjPmn_7p41"
   },
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# global variables\n",
    "DATA_FILENAME = 'CCPP/Folds5x2_pp.xlsx'\n",
    "FEATURE_NAMES = ['AT', 'V', 'AP', 'RH']\n",
    "LABEL_NAME = 'PE'\n",
    "# Load the data from the excel file\n",
    "def load_data():\n",
    "    def split_feature_label(data_set):\n",
    "        features = data_set[FEATURE_NAMES]\n",
    "        labels = data_set[LABEL_NAME]\n",
    "        return features, labels\n",
    "\n",
    "    data = pd.read_excel(DATA_FILENAME)\n",
    "    train_set, dev_set, test_set = data[:5000], data[5000: 7000], data[7000:]\n",
    "\n",
    "    train_features, train_labels = split_feature_label(train_set)\n",
    "    dev_features, dev_labels = split_feature_label(dev_set)\n",
    "    test_features, test_labels = split_feature_label(test_set)\n",
    "\n",
    "    return train_features, train_labels, dev_features, \\\n",
    "        dev_labels, test_features, test_labels\n",
    "\n",
    "\n",
    "# preprocess (by z-normalization) the data for the regression task\n",
    "# return the normalized feature sets and corresponding target variables \n",
    "def prepare_load_regression_data():\n",
    "    train_features, train_labels, dev_features, \\\n",
    "        dev_labels, test_features, test_labels = load_data()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(train_features)\n",
    "    train_features = pd.DataFrame(data=scaler.transform(train_features), columns=FEATURE_NAMES)\n",
    "    dev_features = pd.DataFrame(data=scaler.transform(dev_features), columns=FEATURE_NAMES)\n",
    "    test_features = pd.DataFrame(data=scaler.transform(test_features), columns=FEATURE_NAMES)\n",
    "\n",
    "    return train_features, train_labels, dev_features, \\\n",
    "        dev_labels, test_features, test_labels\n",
    "\n",
    "# binarize the data for the classification task\n",
    "# return the discretized feature sets and corresponding target variables \n",
    "def prepare_load_classification_data():\n",
    "    train_features, train_labels, dev_features, \\\n",
    "        dev_labels, test_features, test_labels = load_data()\n",
    "    feature_mean, label_mean = train_features.mean(axis=0), train_labels.mean(axis=0)\n",
    "\n",
    "    train_features = pd.DataFrame(data=np.where(train_features > feature_mean, 1, 0), columns=FEATURE_NAMES)\n",
    "    dev_features = pd.DataFrame(data=np.where(dev_features > feature_mean, 1, 0), columns=FEATURE_NAMES)\n",
    "    test_features = pd.DataFrame(data=np.where(test_features > feature_mean, 1, 0), columns=FEATURE_NAMES)\n",
    "    train_labels = pd.DataFrame(data=np.where(train_labels > label_mean, 1, 0), columns=[LABEL_NAME])\n",
    "    dev_labels = pd.DataFrame(data=np.where(dev_labels > label_mean, 1, 0), columns=[LABEL_NAME])\n",
    "    test_labels = pd.DataFrame(data=np.where(test_labels > label_mean, 1, 0), columns=[LABEL_NAME])\n",
    "\n",
    "    return train_features, train_labels, dev_features, \\\n",
    "        dev_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QabF2JOdMTI4"
   },
   "source": [
    "## Training and Interpreting a Linear Regression Model\n",
    "\n",
    "**Q1**. Train a linear regression model (we recommend the statsmodels package) and report $R^2$ (goodness of fit) statistic. \n",
    "\n",
    "For model interpretability, provide for each feature (+ the bias variable) the following in tabular format: \n",
    "* Weight estimates\n",
    "* SE (standard error of estimates) \n",
    "* T statistics \n",
    "\n",
    "\n",
    "Further Questions regarding the linear model (to be included in the report): \n",
    "\n",
    "**Q2**. Which three features are the most important?\n",
    "\n",
    "**Q3**. How does the gas turbine energy yield (EP) change with unit (one degree C) increase of the ambient temperature given that all other feature values remain the same?\n",
    "\n",
    "**Q4**. Visualize the weight estimates using 95% confidence intervals.\n",
    "\n",
    "**Q5**. Show bar graph illustrations of the feature effects for the first two validation set instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels, dev_features, \\\n",
    "        dev_labels, test_features, test_labels = prepare_load_regression_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse_test_set(model_lr,params,X_test,test_labels):\n",
    "    predictions = model_lr.predict(params,X_test)\n",
    "    mse = np.mean(np.square(predictions-test_labels).values)\n",
    "    return mse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "B91BszFhMStw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     PE   R-squared:                       0.929\n",
      "Model:                            OLS   Adj. R-squared:                  0.929\n",
      "Method:                 Least Squares   F-statistic:                 1.644e+04\n",
      "Date:                Thu, 27 May 2021   Prob (F-statistic):               0.00\n",
      "Time:                        16:29:42   Log-Likelihood:                -14635.\n",
      "No. Observations:                5000   AIC:                         2.928e+04\n",
      "Df Residuals:                    4995   BIC:                         2.931e+04\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        454.4443      0.064   7108.416      0.000     454.319     454.570\n",
      "AT           -14.8080      0.156    -95.152      0.000     -15.113     -14.503\n",
      "V             -2.8534      0.127    -22.543      0.000      -3.102      -2.605\n",
      "AP             0.3820      0.077      4.948      0.000       0.231       0.533\n",
      "RH            -2.4247      0.083    -29.185      0.000      -2.588      -2.262\n",
      "==============================================================================\n",
      "Omnibus:                      498.725   Durbin-Watson:                   2.030\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2485.693\n",
      "Skew:                          -0.355   Prob(JB):                         0.00\n",
      "Kurtosis:                       6.380   Cond. No.                         4.85\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "R2\n",
      "0.9294006278833447\n",
      "Weight estimates\n",
      "const    454.444328\n",
      "AT       -14.807970\n",
      "V         -2.853379\n",
      "AP         0.382037\n",
      "RH        -2.424737\n",
      "dtype: float64\n",
      "SE\n",
      "const    0.063930\n",
      "AT       0.155624\n",
      "V        0.126574\n",
      "AP       0.077210\n",
      "RH       0.083081\n",
      "dtype: float64\n",
      "T-statistics\n",
      "const    7108.416217\n",
      "AT        -95.152274\n",
      "V         -22.543124\n",
      "AP          4.948008\n",
      "RH        -29.185187\n",
      "dtype: float64\n",
      "Error of the model 289.2259112906741\n",
      "Important features with the most important being at the top\n",
      "AT \t 21.115801458165734\n",
      "V \t 1.7652767537230194\n",
      "RH \t 1.5020929236891531\n",
      "AP \t 1.0044579790751593\n",
      "On AT 1 C increase EP increases by: 14.807970067235226 Probably not the right way to do this look into PDP + ICE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.YTick at 0x13d46300108>,\n",
       "  <matplotlib.axis.YTick at 0x13d462b58c8>,\n",
       "  <matplotlib.axis.YTick at 0x13d461bdb48>,\n",
       "  <matplotlib.axis.YTick at 0x13d4646b7c8>],\n",
       " [Text(0, 0, 'AT'), Text(0, 1, 'V'), Text(0, 2, 'AP'), Text(0, 3, 'RH')])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We recommend the statsmodels package\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train = train_features.copy(deep=True)\n",
    "X_test = test_features.copy(deep=True)\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "model_lr = sm.OLS(train_labels, X_train)\n",
    "\n",
    "res = model_lr.fit()\n",
    "\n",
    "print(res.summary())\n",
    "print(\"R2\")\n",
    "print(res.rsquared)\n",
    "r2 = res.rsquared\n",
    "print(\"Weight estimates\")\n",
    "print(res.params)\n",
    "print(\"SE\")\n",
    "print(res.bse)\n",
    "print(\"T-statistics\")\n",
    "print(res.tvalues)\n",
    "print(\"Error of the model\",res.mse_total)\n",
    "feature_importance = []\n",
    "test_mse = calculate_mse_test_set(model_lr,res.params,X_test,test_labels)\n",
    "\n",
    "for feature in FEATURE_NAMES:\n",
    "    augmented_test_set = test_features.copy(deep=True)\n",
    "    augmented_test_set[feature] = augmented_test_set[feature].sample(frac=1).reset_index(drop=True)\n",
    "    augmented_test_set = sm.add_constant(augmented_test_set)\n",
    "    aug_test_mse = calculate_mse_test_set(model_lr,res.params,augmented_test_set,test_labels)\n",
    "    feature_importance.append((feature,aug_test_mse/test_mse))\n",
    "#     print(feature,'importance:',)\n",
    "    \n",
    "feature_importance = sorted(feature_importance,key=lambda x: x[1],reverse=True)\n",
    "print(\"Important features with the most important being at the top\")\n",
    "for fi in feature_importance:\n",
    "    print(fi[0],'\\t',fi[1])\n",
    "\n",
    "pred_list= []    \n",
    "for aug in [0,1]:\n",
    "    augmented_test_set = test_features.copy(deep=True)\n",
    "    if aug:\n",
    "        augmented_test_set['AT'] = augmented_test_set['AT']+1\n",
    "    augmented_test_set = sm.add_constant(augmented_test_set)\n",
    "    predictions = model_lr.predict(res.params,augmented_test_set)\n",
    "    pred_list.append(predictions)\n",
    "print(\"On AT 1 C increase EP increases by:\",np.mean(pred_list[0]-pred_list[1]),\"Probably not the right way to do this look into PDP + ICE\")\n",
    "ci = res.conf_int()\n",
    "\n",
    "# plt.bar(confidence_intervals,100)\n",
    "# plt.show()\n",
    "\n",
    "data_dict = {}\n",
    "data_dict['features'] = FEATURE_NAMES\n",
    "data_dict['lower'] = ci[0].values[1:]\n",
    "data_dict['upper'] = ci[1].values[1:]\n",
    "dataset = pd.DataFrame(data_dict)\n",
    "\n",
    "for lower,upper,y in zip(dataset['lower'],dataset['upper'],range(len(dataset))):\n",
    "    plt.plot((lower,upper),(y,y),'ro-',color='orange')\n",
    "plt.yticks(range(len(dataset)),list(dataset['features']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tj6Pri4HBeO"
   },
   "source": [
    "**Q6.** Reflection: why would training a regression tree not work well for this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiv5chyDOfxS"
   },
   "source": [
    "## Training and Interpreting Classification Models\n",
    "Using the preprocessing function implemented above to prepare the dataset for  the classification task. This function simply binarizes all variables including the target variable (EP) using the respective training set mean as threshold. A value of 1 means a high value vs 0 a low(er than average) value. Note that we do the feature binarization to ease interpretation of the models, normally that is not necessary for classification models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7r09mMfeo2k"
   },
   "source": [
    "### Training and Interpreting EBM\n",
    "Train a Explainable Boosting Machine (with [interpret.ml](https://github.com/interpretml/interpret/))\n",
    "\n",
    "For a tutorial see: [[Tutorial](https://nbviewer.jupyter.org/github/interpretml/interpret/blob/master/examples/python/notebooks/Interpretable%20Classification%20Methods.ipynb)]\n",
    "\n",
    "**Q7**. Report (global) feature importances for EBM as a table or figure. What are the most important three features in EBM? Are they the same as in the linear model? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-ZmqpxweoZv"
   },
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "# EBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_k7dAwTIfbsc"
   },
   "source": [
    "### Training and Explaining Neural Networks\n",
    "Train two Neural Networks:\n",
    "1. One-layer MLP (ReLU activation function + 50 hidden neurons)\n",
    "2. Two-layer MLP (ReLU activation function + (20, 20) hidden neurons)\n",
    "\n",
    "We recommend to use the Adam optimizer. Fine-tune the learning rate and any other hyper-parameters you find necessary. \n",
    "\n",
    "For a tutorial see: [[Tutorial](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQjg_qtCf_WD"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# One-layer MLP\n",
    "\n",
    "# Two-layer MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpGv6J6XlBQ4"
   },
   "source": [
    "You can check the tutorials for SHAP and LIME explanations for neural networks \n",
    "[[SHAP Tutorial](https://nbviewer.jupyter.org/github/interpretml/interpret/blob/master/examples/python/notebooks/Explaining%20Blackbox%20Classifiers.ipynb)] \n",
    "[[LIME Tutorial](https://nbviewer.jupyter.org/github/interpretml/interpret/blob/master/examples/python/notebooks/Explaining%20Blackbox%20Classifiers.ipynb)]\n",
    "\n",
    "\n",
    "**Q8**. Provide explanations for randomly selected three test set instances using two explanation methods (LIME and SHAP) with two NN models  (namely the single-hidden layer NN model and the two-hidden-layer NN model: 2 x 2 x 3 = 12 explanations in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIo-o_lClJYQ"
   },
   "outputs": [],
   "source": [
    "# Global explanations\n",
    "import graphviz\n",
    "from interpret import show\n",
    "\n",
    "# Local explanations (SHAP and LIME)\n",
    "from interpret.blackbox import LimeTabular\n",
    "from interpret.blackbox import ShapKernel"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of programming_assignment_2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
